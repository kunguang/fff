## 基于无模型的强化学习方法

基于蒙特卡洛的方法

同策略:
[同策略和异策略](https://www.zhihu.com/question/57159315)

优点:
使用人类的行为来作为behavior policy，直接从人类学习以 旧policy 作为behavior policy 学习到的知识使用一些重视“探索”的policy，如Q-learning的  - greedy policy来作为behavior policy

### 蒙特卡洛
关键问题:
1.如何获得充足的经验是无模型强化学习的核心所在
2.有模型的时候，动态规划是扫描所有的状态。所以无模型的要保证每个状态都被访问到
背景和问题:

前面的动态规划，都是因为有状态-行为转移矩阵，但是事实上大部分的时候，我们并不直到。但是在前面我们也确定了强化学习的框架，策略评估和策略改善。如果没有状态转移矩阵，我们该怎么做策略评估?

解决办法:

1.得到P。使用蒙特卡洛算法，不断的产生样本，或者产生很多片段，最后如果数据量多，其实也可以统计出来P，但是在产生这个P的过程中，我们也能完成状态转移矩阵。现在的这些解决办法都是针对查询表设置的。如果状态或者action是无穷多的那就没有意义了。
2.得到回报。衡量每个状态的回报使用最原始的折扣因子计算方法，没法使用求期望的方法。

具体解决办法:
策略评估：
1.求若干个样本的均值。这也是蒙特卡洛的基本原理。统计每个(s,a)的值。
策略改善：
1.获取max(s,a)
问题:上面求均值的目的是说，所有的回报，或者所有的实验路径，概率都是一样大。但是其实不然。如果结合实际问题。假设现在蒙特卡洛采用异策略的办法，那么我们的本来的目标是要求解每个(s,a)下的值，本来我们要改善的目标是Pi,但是其实在产生行为策略或者实验路径是u产生的，相当于我们用u去拟合了p。
![](assets/markdown-img-paste-20180813202134709.png)
![](assets/markdown-img-paste-20180801080651422.png)

![](assets/markdown-img-paste-20180801080753249.png)
比较关键的一步是，因为我们的Pi,本来是贪心策略，所以P一定是1，某个状态下选取某个action的概率一定是1，其实这个很容易被模拟，不就是每个状态都有固定值。但是我们就是想要异策略，导致有另外一个概率分布的存在，再看重要性采样，分子是pi的概率，所以一旦我们的u某次采样的概率分布，或者某个action不是最佳的action,那就没必要继续下去了。
这儿存在一个迷惑的点:那个退出for循环的条件为什么是在策略改善之后呢。这时候其实我们的pi是有两个取值，改善之前和改善之后，我们最终的目标是改善之后的之后，pi使我们的最终目标，所以策略改善之后的pi更接近我们的目标，所以我们一旦发现此时概率不一样，按照正常的概率计算公式，当前状态之前的状态值没必要更新了，因为接下来的状态对于以前的状态值都是有偏的。这儿也是有个小问题，就是不管怎么样倒数第二个状态，不管action是否一样都要更新。因为总是要更新某一步的状态的，要不然永远不可能进步。只能说评估之后，才能做改善。这他么也会造成浪费，相当于这次遍历是无用的，尤其是刚开始，这种修改大部分时候都是显而易见的。因为不断改善的目标就是为了不断修改pi.
### 基于时间差分的强化学习方法
动态规划的好处，是实时性比较强，即每走一步都可以更新一次模型，因为可以立即得到回报。背后的原因是自举性
蒙特卡洛的的好处是解决无模型的问题，但缺点是，如果根据最原始的回报计算公式，那么每次都要等到一个片段结束。这对于实时性要求强的就不那么友好了，举个不友好的观点，非得等到某个车祸真正发生后再去更新模型，代价太大。

三者之间的关系
![](assets/markdown-img-paste-20180801080932884.png)
怎们样把上面两个结合，利用自举法实时性的解决无模型的问题。
整个强化学习最为关键的地方:
![](assets/markdown-img-paste-2018080108103013.png)
1.同策略的sarsa
这个是要走到T+1步才能更新第T步的内容
![](assets/markdown-img-paste-20180801081238184.png)
行动和评估策略都是采用的带有随机性的策略，都不是贪婪策略
2.异策略的Q-learning。
实时性的更新，走完就更新。
![](assets/markdown-img-paste-20180801081316108.png)
行动策略是随机策略，评估策略是贪婪性策略。
可以进一步看看TD(lambda)算法。
提出的前提，因为上面两个方法都是一步更新，实时的更新值，得到的结果方差小，但是离期望远，所以是不是能找到一个更好的办法，能够解决期望偏差问题。很自然的想法，我们也效仿蒙特卡洛，使用n步，即截止到最后一个片段。但是又不想方差太大，所以给予越靠前的状态权重越大，越靠后的权重越小。毫无疑问方差肯定会增大些。这个时候就考虑方差和期望的均衡了
![](assets/markdown-img-paste-2018081320435751.png)
![](assets/markdown-img-paste-20180801081345519.png)
前向理解:
可以忽略，跟蒙特卡洛一样。
后向理解:
这个理解，我感觉很精单，就是到达了某个状态之后，告诉之前经历过的所有状态，我更新了，你们有用到我的也要跟着更新。这就可以实时的解决问题了。
将每一项展开，消掉某一些公式。
![](assets/markdown-img-paste-20180801081412394.png)
上面的G是最终的值，V是初始值，下面的结果可以看做经历这一个片段之后，V(St)的增长值。这样就能够设计函数了
![](assets/markdown-img-paste-20180801081443135.png)
上面有个错误的地方，E其实就是一直累加1的数值，是前面Lambda的指数项。
### 基于值函数的逼近法
1.前面的能够用表格表示的Q，其实状态空间和行为空间都是离散的。但如果状态空间是连续的呢，这个表格就是无穷大的。我们该怎么表示呢。所以此时就引入参数表示法。用参数去拟合每个状态的回报。
![](assets/markdown-img-paste-20180801081945273.png)
此时可以将前面的G当做Label,后面的Q当做正常的样本。
优化算法，可以采用随机梯度更新。构造数来有监督的样本，就按照那套优化流程了
![](assets/markdown-img-paste-20180801082023872.png)
一般也都是半梯度算法，因为label值和样本值都用到了参数，理论上label是真实值，不应该有参数

### DQN

就是用神经网络去拟合样本参数。缺点:不稳定性，根本原因是数据之间的依赖性太强，前后关联性太大。所以要打破这种关联性。解决办法:
1.回放和均匀抽取数据，也有可能有权重抽取(如果某个样本的误差很大，被抽取的概率很大)。也有可能有其他的办法。
2.上面的半梯度法其实挺怪的，我们可以进一步拆解，TD参数和值函数参数是两个不同的值。如果他们俩相同会增加数据之间的关联性，可以这样理解，最终的目标都是拟合目标值，如果目标值参数和当前的值函数参数都是一样的，就等价于用当前的参数拟合当前的参数值，当前参数的更新以及目标值的更新其实都是受制于当前的样本回报数据。但是如果TD参数一直使用若干步之前的数据，至少目标值参数不受制于当前的数据。这种异策略的其实都是在利用旧信息。
![](assets/markdown-img-paste-20180801082215775.png)
### double dqn.
这个也没啥大惊小怪的，因为我们使用Q-learning,每次都选使得值最大的那个action的值，这样导致每一次目标函数的预估值都是变大的，偏差变大，如果全部都是成比例变大的没问题，但是事实肯定不会是成比例的。如果我们改用当前一直更新的参数去选择哪个action导致的值最大，然后选取action后，再使用旧网络预估当前action的值，作为当前状态的值。

4.优先回放
在抽取数据的时候，均匀抽取会没有重点，比如我们明明知道TD误差大的样本能给我们带来的受益最大，我们应该优化这些，优化那些没有变化的没什么意义。
这种抽取方法会有很多种。
1.按照td误差计算每个样本的概率，用softmax。然后从这里面随机抽取。
2.按照td误差排序。1/rank(i)。
矫正上面的误差，因为上面的分布肯定和真是分布不一样使用(1/(N*pi))^beta 这种属于降低某个样本的权重，如果某个样本被选取的概率越大，这个值就越小，如果beta为1，可以这样理解，如果Pi是0.1,N是100，首先这样本被抽中的概率0.1,这个1/(N*pi)=1/10=0.1,这样这个样本的权重就是1%，正常，这个样本本来就是100个钟的一个，概率就是1%.如果beta越大，说明样本权重越小。实际上beta值刚开始是0.4，慢慢递增到1.刚开始这种样本的权重就是大。
来看另外一种抽样办法:
1.使用二叉树，应该是类似于堆那种形式，最底层存储的是每个样本的误差值,父节点存储的是子节点的和。刚开始子节点都是1，如果第一次选取一批样本后，这时候他们的TD残差也就被更新了，一般残差是小于1的，总之目的就是使得被选取过的样本的值尽可能小于没有被选取的，被选取的里面残差大的还是比较大。
![](assets/markdown-img-paste-20180801082313389.png)
4,批处理更新每个样本的值
这儿会控制所有的误差小于1，那些没有被改变过的都是1，就是说但凡更新的都是小于1的，他们再次被更新的机会就比较小了。优先选取那些没有被更新过的。
在这些被更新过的里面选取更好一些的，下面会让这些变大一些，就是为了更加接近1.使得这次误差较大的样本，也有机会和那些原本没有被更新过的可以比拼一下。
这会引入另外一个问题，这个误差多大是合适的，跟回报函数有关系了。
